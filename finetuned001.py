# -*- coding: utf-8 -*-
"""FineTuned001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUxh6ZKkXhrvMHOzCXyicZP59uUhOXaF
"""

print("hello world")

!pip install peft accelerate bitsandbytes transformers datasets

!pip install GPUtil

import torch
import GPUtil
import os

GPUtil.showUtilization()

if torch.cuda.is_available():
  device = torch.device("cuda")
  print("GPU is available")
else:
  device = torch.device("cpu")
  print("GPU is not available")


os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"

import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM , BitsAndBytesConfig ,LlamaTokenizer
from peft import prepare_model_for_kbit_training , LoraConfig , get_peft_model
from datasets import load_dataset
from huggingface_hub import notebook_login

# from google.colab import userdata
# userdata.get(' YOUR_Huggingface_API_key')

base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config
)

!git clone https://github.com/poloclub/Fine-tuning-LLMs.git

train_datset = load_dataset("text" , data_files={"train": ["/content/Fine-tuning-LLMs/data/hawaii_wf_4.txt","/content/Fine-tuning-LLMs/data/hawaii_wf_2.txt"]} , split="train")

train_datset["text"][0]

tokenizer = LlamaTokenizer.from_pretrained(base_model_id , use_fast= False , trust_remote_code=True , add_eos_token=True)

if tokenizer.pad_token_id is None:
  tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})

tokenized_train_dataset =[]

for phrase in train_datset:
  tokenized_train_dataset.append(tokenizer(phrase["text"]))

tokenized_train_dataset[1]

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

config = LoraConfig(
    r=8,
    lora_alpha=64,
    target_modules=["q_proj" , "k_proj" , "v_proj" , "o_proj " , "up_proj " ,"down_proj "  ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model , config)

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    args=transformers.TrainingArguments(
        output_dir="./finetunedModel",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        num_train_epochs=3,
        learning_rate=1e-4,
        max_steps=20,
        bf16=False,
        optim="paged_adamw_8bit",
        logging_dir="./log",
        save_strategy="epoch",
        save_steps=50,
        logging_steps=10

),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache=False
trainer.train()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig, LlamaTokenizer
from peft import PeftModel

base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

nf4Config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=nf4Config,
    device_map="auto",
    trust_remote_code=True,
    use_auth_token=True
  )

tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True

                              )

modelFinetuned = PeftModel.from_pretrained(base_model, "finetunedModel/checkpoint-20")

user_question = "When did Hawaii wildfires start?"

eval_prompt = f"Question: {user_question} Just answer this question accurately and concisely.\n"

promptTokenized = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

modelFinetuned.eval()

with torch.no_grad():
  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))
  torch.cuda.empty_cache()

